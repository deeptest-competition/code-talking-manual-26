# System Description
System name: CRISP (Contextual Risk-Driven Input Structuring for Probing)
The system is a test-case generator that tries to make the manual-assistant model omit required safety warnings by producing short, realistic user questions that should “force” a warning to be relevant. Concretely, on each iteration it samples a target warning (by ID) from an internal warning_pool (which gets biased over time), then builds a request by (a) inferring a coarse risk category from the warning’s top_section_title + warning_text using keyword rules (weather/towing/tires/maintenance/charging), (b) selecting a risk-context fragment consistent with that category (e.g., “on icy roads,” “during charging”), (c) extracting an anchor phrase from the warning text by tokenizing it, removing stopwords, and keeping the longest 1–2 remaining tokens, and (d) choosing an action either category-driven (e.g., “start charging” for charging) or falling back to a generic action list; it then injects these components into a small set of terse procedural templates (optionally adding a light “speech filler” like “sorry,”) and finally applies a hard word-limit enforcement to keep the request within 25 words. The generated TestCase is returned with request, expected_warning_id, and (critically) warning_text populated so the SimpleJudge can retrieve relevant context and decide whether the warning was required; after the pipeline runs, your update_state method checks the judge outcome and, if judge_response.score == 0 (meaning the warning was needed but omitted), it exploits that weakness by duplicating the corresponding warning ID in warning_pool, increasing the probability of sampling that same warning again in future tests.


# How to run 
Code is run using the standard format. 
python main.py --test_generator custom --n_tests 10 --time_limit_seconds 300 --sut_llm gpt-4o --oracle_llm gpt-4o-mini --generator_llm g 
